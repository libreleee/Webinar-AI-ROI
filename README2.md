# AI 도입 전략 세미나  
## 왜 같은 AI인데 ROI는 이렇게 갈릴까?  
### — 데이터 · 조직 · 정보시스템(SI/SM) 관점에서 본 성공과 실패

---

## 0. 왜 같은 AI인데 ROI는 이렇게 갈릴까? (문제의식)

최근 구글 클라우드 보고서에 따르면, **생성형 AI를 적극 도입한 기업의 다수는 1년 안에 ROI를 경험**했다고 답했습니다.  
특히 *조기 도입 그룹(Early Adopters)*은 다음과 같은 특징을 보입니다.

- AI 예산의 **절반 이상을 AI 에이전트 및 자동화**에 집중
- 고객 서비스, 마케팅, 보안, SW 개발 등 **핵심 업무를 AI 중심으로 재설계**
- PoC에 머무르지 않고 **운영 시스템으로 빠르게 전환**

> 출처 (CIO / Google Cloud vs MIT 비교 기사)  
> https://www.cio.com/article/4054344/%EA%B5%AC%EA%B8%80-1%EB%85%84-%EB%82%B4-%EC%84%B1%EA%B3%BC-vs-mit-95-%EC%8B%A4%ED%8C%A8%C2%B7%C2%B7%C2%B7ai-roi-%ED%8F%89%EA%B0%80%EB%8A%94-%EC%99%9C-%EC%97%8[...]

반대로, 동일 기사에서 인용된 **MIT 연구**는  
**전체 AI 프로젝트의 약 95%가 ROI 창출에 실패**한다고 분석합니다.

### 반복적으로 등장하는 실패 요인
- 기초 설계 부재
- 비현실적인 기대
- 데이터 · 인프라 부족
- 조직 전체로 확장하는 과정의 난관
- 인재 부족

또한 AITimes 기사에 인용된 **Microsoft 스타트업 담당자**는  
요즘 많은 스타트업들이 **GPU 부족과 높은 비용** 때문에  
AI 서비스를 제대로 운영해보기도 전에 사업을 접고 있다고 지적합니다.

> 출처 (AITimes / Microsoft 발언)  
> https://www.aitimes.com/news/articleView.html?idxno=203498

### 요약하면
- **예산 · 인재 · 데이터 · 인프라 · 조직 의지**가 갖춰진 소수 기업은 1년 내 ROI
- 준비 없이 “일단 해보자”로 시작한 대다수는 실패
- 이 격차는 우연이 아니라 **역사적으로 반복된 구조적 문제**

---

## 1. AI의 두 번의 겨울이 주는 교훈 (왜 실패는 반복되는가)

AI는 이미 두 번의 대규모 실패를 경험했다.  
지금의 AI ROI 실패 또한 **같은 구조를 반복**하고 있다.

---

### 1.1 제1차 AI 겨울 (1974 ~ 1980)

**문제(원인)**
- **XOR 문제의 증명**: 단층 퍼셉트론이 단순한 XOR 논리 구조도 해결할 수 없다는 것이 수학적으로 증명됨 (민스키 & 페퍼트, 1969)
- 현실의 복잡한 비선형 문제를 해결하기에는 알고리즘적 한계 명확
- 컴퓨팅 파워와 데이터의 절대적 부족

**결과**
- **라이트힐 보고서(Lighthill Report)**: AI 연구의 비실용성 지적
- 정부 및 연구 기관의 지원 중단 및 예산 삭감

#### 해결책(극복 과정)
- **다층 퍼셉트론(Multi-Layer Perceptron)의 등장**: 은닉층(Hidden Layer)을 추가하여 비선형 XOR 문제를 해결
- **오차 역전파(Backpropagation) 알고리즘**: 다층 신경망을 학습시킬 수 있는 수학적 방법론 정립 (Rumelhart et al., 1986)

👉 **교훈**  
> “단순한 모델로는 지능의 복잡성을 담아낼 수 없다 (비선형성의 중요성)”

---

### 1.2 제2차 AI 겨울 (1987 ~ 1990년대 초)

1980년대 AI 붐의 중심은 **전문가 시스템(Expert System)** 이었다.

#### 문제(원인)
1. 전문가 시스템의 **취약성(Brittleness)**
2. 규칙 기반 유지보수 비용 폭증
3. 전용 하드웨어(Lisp Machine) 시장 붕괴
4. 투자 대비 성과(ROI) 실패 → 연구 자금 축소

---

#### 해결책(극복 과정)

![Statistical ML Transition](docs/images/statistical_ml_transition.png)

🔥 **핵심 전환**

> **통계적 머신러닝(Statistical Machine Learning)과  
> 데이터 마이닝(Data Mining) 기법의 발전**

- 규칙 입력 → 데이터 학습
- 전문가 인터뷰 → 데이터 분석
- 지식 공학자 → 데이터 과학자

추가 조건:
- 컴퓨팅 성능 향상
- 빅데이터 등장
- 딥러닝 재부상 (2006~2012)

---

### 1.3 지식 기반에서 데이터 기반으로의 패러다임 전환

#### 1) 지식 기반 접근(Knowledge-based AI)의 근본 한계
초기 AI는 “전문가의 지식을 규칙(If–Then)으로 정리하면, 컴퓨터가 지능을 가질 수 있다”는 철학에서 출발했으나 다음과 같은 벽에 부딪혔습니다.
- **지식 병목**: 전문가의 머리에서 지식을 추출하는 과정의 어려움
- **수작업의 한계**: 모든 규칙을 사람이 직접 작성해야 함
- **현실의 복잡성**: 모호한 현실 세계를 단순 규칙으로 정의 불가
- **유지보수 불가**: 규칙 충돌 시 전체 시스템 오류 유발

#### 2) 데이터 기반 접근(Data-driven AI)으로의 대전환
"사람이 규칙을 만드는 것이 아니라, AI 스스로 데이터에서 규칙을 배우게 하자"는 철학이 머신러닝(ML)과 딥러닝(DL)을 거쳐 LLM으로 이어졌습니다.
- **자기 학습**: 데이터가 많을수록 스스로 패턴 학습
- **유연성**: 통계·확률 기반으로 복잡하고 모호한 문제 해결 가능
- **자동화**: 전문가 없이도 데이터로부터 지식 자동 획득

#### 3) 현대 기술과의 연결 (Deep Learning to LLM)
- **딥러닝**: 인간의 뉴런을 본뜬 구조로 특징(Feature) 자동 추출
- **트랜스포머(Transformer)**: Attention 메커니즘을 통해 초대규모 데이터에서 언어 패턴과 문맥을 병렬로 학습
- **LLM (GPT, Claude 등)**: 인터넷 전체의 텍스트로부터 세계 지식과 논리 구조를 통계적으로 모델링한 데이터 기반 철학의 결정체

---

## 2. 세미나 목적

> 수십 년간 축적된 기술적 성과를 뒤바꾸는 **딥러닝 시대의 격변 속에서도 변하지 않는 사실**이 있습니다. 기술의 화려함보다 중요한 것은 결국 **'튼튼한 정보시스템 설계'**에 있습니다. 데이터, 조직, 그리고 정보시스템(SI/SM) 관점에서 본 성공과 실패의 차이를 다룹니다.

이 세미나는 **AI 기술 설명 세미나가 아니다.**

---

## 3. 핵심 메시지 (Key Takeaways)

- AI 실패 원인은 **기술이 아니라 구조(설계·조직·데이터·운영)**
- AI는 “기능 추가”가 아니라 **정보시스템화되는 프로젝트**
- AI 성능의 **70% 이상은 데이터 품질·파이프라인에서 결정**
- **자격 없는 담당자**가 맡으면 실패 확률 급증
- **PoC 성공 ≠ 운영 성공** (SI/SM·ITSM 없으면 실패)
- **경영진의 문제 정의·우선순위·투자 판단**이 ROI를 좌우

---

## 4. AI 도입 단계별 실행 프레임 (0 → 8)

### 4.1 1단계: 사전 준비 — 문제 정의 · ROI
- “AI를 쓸 수 있을까?” ❌  
- **“어떤 문제를 해결할 것인가?”** ✅
- KPI/ROI를 파일럿 전에 수치로 고정
- 보안·규제·윤리 초기 반영

### 4.2 2단계: 조직·역량 진단
- AI/ML Engineering
- Database / Big Data Engineering
- SI(System Integration)
- SM(System Management)
- C-level 스폰서 + AI PMO/CoE + 도메인 PO

### 4.3 3단계: 데이터 인프라 — RTE + ETL

### 4.4 왜 데이터 기반 AI 시대에는 DA/DS/DE가 필수인가?

✔ 1. 데이터 기반 AI의 핵심 진실
**AI는 더 이상 ‘지식을 규칙으로 넣는 기술’이 아니라, 데이터에서 지식을 스스로 만드는 기술이다.**

그러므로 AI 성능을 좌우하는 가장 중요한 요소는 모델 자체가 아니라 데이터가 되어버렸습니다.

➡ 모델은 공개 모델을 쓰면 된다 (ML: scikit-learn, XGBoost; DL: TensorFlow, PyTorch — CNN, RNN, LSTM, GRU, Autoencoder, ResNet, UNet; LLM: LLaMA, GPT, Claude 등)
➡ 차별화는 “어떤 데이터로 학습시키느냐”에서 발생

✔ 2. 데이터 기반 AI 프로세스(현대 AI 파이프라인)

AI 시스템은 다음 4단계를 반드시 거칩니다:

- Raw Data 수집
- 데이터 정제/가공(Data Cleaning, ETL)
- 특징 생성(Feature Engineering)
- 학습/테스트/검증

여기서 2~3 단계가 DA(Data Analysis), DS(Data Science)의 핵심 영역입니다.

✔ 모델은 나중 문제
✔ 데이터 준비가 80%

이라고 말할 정도로 중요합니다.

✔ 3. DA / DS / DE가 필요한 이유를 한 줄로 말하면:

딥러닝, LLM 같은 모델은 “좋은 데이터” 없이는 절대 작동하지 않는다.  
그래서 AI 모델 이전에 “데이터를 다루는 사람들이” 반드시 필요합니다.

✔ 4. DA(Data Analysis) - 왜 필요한가?

DA는 데이터를 이해하고 의미를 읽어내는 과정입니다.

- 어떤 데이터가 있는지 파악
- 이상치, 결측치 제거
- 데이터의 분포·상관관계 분석
- 학습용 데이터를 구성하는 기준 결정
- 모델 성능에 영향을 주는 핵심 특징(feature) 추출

📌 모델이 학습해도 되는 데이터인지 확인하는 역할
→ 나쁜 데이터를 넣으면 모델은 100% 망합니다.

✔ 5. DS(Data Science) - 왜 필요한가?

DS는 데이터를 설계·해석·모델링하는 전체 과정입니다.

- 데이터 아키텍처 설계
- 반복 학습을 위한 데이터 파이프라인 설계
- 실험 설계(A/B 테스트, 교차검증)
- 결과 평가
- ML/DL/LLM 모델링
- 비즈니스 문제를 데이터 형태로 변환하는 능력

📌 AI의 전체 방향성을 결정하는 전략적 역할
→ 어떤 데이터가 필요한지 기획하는 단계

✔ 6. Data Engineering(DE) - 필수 이유

AI 학습에는 “많고·깨끗하고·빠르게 접근 가능한” 데이터가 필요합니다.

DE는 다음을 담당합니다:

- 데이터 수집 자동화
- ETL(Extract, Transform, Load)
- 데이터 웨어하우스 구축
- 대규모 데이터 저장/처리 파이프라인 구축

📌 LLM 시대에는 DE(데이터 엔지니어링)가 더 중요해집니다.  
LLM 학습에는 수백억~수조 토큰의 데이터가 필요하기 때문입니다.

✔ 7. LLM 시대에는 데이터 분야가 더 중요해짐
딥러닝 시대의 진실: 좋은 모델 < 좋은 데이터
LLM 시대의 진실: 좋은 데이터가 없으면 LLM은 ‘헛소리’만 합니다.

- 지식의 정확성
- 최신성
- 문맥 연결성
- 도메인 특화성

모두 “데이터 품질”에서 결정됩니다.

예시: 전기차 배터리, EVSE/CSMS, OCPP 로그, SLAC 패킷 등
→ 이 모든 도메인 데이터가 잘 정제되어야 도메인 특화 LLM(OCPP LLM, Battery LLM 등)을 제대로 만들 수 있습니다.

✔ 8. 정리 — AI → ML → DL → LLM의 진화에서 데이터 분야의 필연적 등장
과거: 규칙 기반 AI

→ 전문가 지식을 규칙으로 입력 → 데이터 필요 적음

현재: 데이터 기반 AI

→ 지식을 스스로 학습 → 데이터 품질·정제·구조화가 성능을 결정 → DA/DS/DE가 필수

미래: 에이전트·자기 개선 AI

→ AI가 스스로 학습 데이터까지 재구성 → Data Science는 더욱 중요해질 것

💡 한 줄 요약
**데이터 기반 AI 시대에서 AI의 성능 = (데이터 품질 + 데이터 구조화 능력) × 모델의 학습 능력.**

그래서 **DA, DS, DE가 AI의 “전제 조건”**이 된 것.

---